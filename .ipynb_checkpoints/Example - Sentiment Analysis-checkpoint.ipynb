{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Accept a search term from the user and download the last 100 tweets with that term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The python-twitter module is used because it has a RESTful API that returns\n",
    "# pre-parsed objects instead of raw JSON to make working with data easier.\n",
    "\n",
    "import twitter\n",
    "import json\n",
    "\n",
    "# The twitter library uses an API object authenticated with app keys to access the API.\n",
    "# For privacy reasons, these keys are loaded from a local JSON file not included in this repo.\n",
    "\n",
    "with open('twitter_keys.json') as keystore:\n",
    "    keys = json.load(keystore)\n",
    "\n",
    "api = twitter.Api(consumer_key=keys['consumer_key'],\n",
    "                  consumer_secret=keys['consumer_secret'],\n",
    "                  access_token_key=keys['access_token_key'],\n",
    "                  access_token_secret=keys['access_token_secret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a function to accept a search term and fetch the tweets with that term.\n",
    "def fetchTestData(search_string):\n",
    "    try:\n",
    "        tweets_fetched = api.GetSearch(search_string, count=100)\n",
    "        print(\"Fetched\",str(len(tweets_fetched)),\"tweets with the term\",search_string)\n",
    "        return [{'text':status.text,'label':None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print(\"Failed to fetch tweets with the term\",search_string)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. Please enter a search term: apple\n",
      "Fetched 96 tweets with the term apple\n"
     ]
    }
   ],
   "source": [
    "search_string = input(\"Hello. Please enter a search term: \")\n",
    "testData = fetchTestData(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trailer for #FiftyShadesDarker features #IDontWannaLiveForever! Check it out below &amp; then pick up the song:… https://t.co/9XrxHNm0xL\n",
      "Hey @Apple, I'm 35. Stop autocorrecting it to ducking. Thanks!\n",
      "Surprise \"Thank You Followers\" Twitter Giveaway! Win I Love Juicy Couture To enter follow @davelackie &amp; RT Taffy ap… https://t.co/Lkaya4upeT\n",
      "@qszell najlepsza opcja to https://t.co/ikM3Q9lpTN i Cortland\n",
      "RT @GSMHProductions: https://t.co/2BTnybR4Ma #cannabiscommunity #RT #Podcast #PodcastTrending #PodernFamily #Therapy #psychology #selfhelp…\n",
      "RT @bekotaizi: taiji hasegawa「PixelEscape」\n",
      "\n",
      "無事iOS版も配信されました（＾ω＾） https://t.co/IGyUOKeICm\n",
      "Qualcomm’s Snapdragon 835 will bring improved battery life and better VR experiences #androbrix1 #android #apple #r… https://t.co/P8RnIeMx0I\n",
      "雷音のゲーム実況 ドラゴンクエストヒーローズ2 2017.01.03 2\n",
      "#マスクドＤＪ雷音 #ドラゴンクエストヒーローズ #ゲーム実況 https://t.co/wflF8WzZup\n",
      "https://t.co/cwD9NAJnWJ\n",
      "Guile: deceit; trickery #word #vocabulary #interesting  https://t.co/RENl8NNdll https://t.co/IEcdM9Fs3y\n"
     ]
    }
   ],
   "source": [
    "# First 10 tweets that were fetched.\n",
    "for tweet in testData[0:9]:\n",
    "    print(tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Classify these tweets as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This requires downloading a corpus of tweet data. However, twitter only allows\n",
    "# tweet ID's to be shared, and not the tweets themselves. The API can be used to\n",
    "# cross-reference the corpus, but considering that Niek Sanders corpus contains\n",
    "# 5000 tweets and twitter limits API pulls to 180/15 mins, it will take several \n",
    "# hours to complete the pull.\n",
    "\n",
    "def createTrainingCorpus(corpusFile,tweetFile):\n",
    "    import csv\n",
    "    corpus=[]\n",
    "    with open(corpusFile,'r') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',',quotechar='\"')\n",
    "        for row in lineReader:\n",
    "            corpus.append({'tweet_id':row[2],'label':row[1],'topic':row[0]})\n",
    "    # The tweet pull is self-throttled to work around twitter's rate limit.\n",
    "    import time\n",
    "    rate_limit=180\n",
    "    sleep_time=900/180 # 15 minutes / limit\n",
    "    trainingData=[]\n",
    "    downloadCount = 1\n",
    "    for tweet in corpus:\n",
    "        try:\n",
    "            status = api.GetStatus(tweet['tweet_id'])\n",
    "            tweet['text'] = status.text\n",
    "            print('fetched tweet',str(downloadCount),'of',str(len(corpus)))\n",
    "            downloadCount += 1\n",
    "            trainingData.append(tweet)\n",
    "            time.sleep(sleep_time) # to avoid rate limit\n",
    "        except:\n",
    "            continue\n",
    "    with open(tweetFile,'w') as csvfile:\n",
    "        lineWriter = csv.writer(csvfile,delimiter=',',quotechar='\"')\n",
    "        for tweet in trainingData:\n",
    "            # Ignore failures on line write so the trainingData doesn't get lost due to a single encoding error.\n",
    "            try:\n",
    "                lineWriter.writerow([tweet['tweet_id'],tweet['text'],tweet['label'],tweet['topic']])\n",
    "            except:\n",
    "                continue\n",
    "    return trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Commented out so 'Run All' doesn't kick off a 10-hour loop.\n",
    "#trainingData = createTrainingCorpus('./corpus.csv','./tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since I don't want to deal with loading 10 hours worth of tweets, this fetches the\n",
    "# training data from my pre-saved csv file.\n",
    "def loadTrainingData(tweetsFile):\n",
    "    import csv\n",
    "    with open(tweetsFile,'r') as dataFile:\n",
    "        lineReader = csv.reader(dataFile, delimiter=',', quotechar='\"')\n",
    "        trainingData = []\n",
    "        for row in lineReader:\n",
    "            if len(row) > 0: # Ignore empty rows, which would cause an 'out of range' indexing error.\n",
    "                trainingData.append({'tweet_id':row[0],'text':row[1],'label':row[2],'topic':row[3]})\n",
    "    return trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData = loadTrainingData('./tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Whit\\Anaconda3\\lib\\site-packages\\nltk\\decorators.py:59: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  regargs, varargs, varkwargs, defaults = inspect.getargspec(func)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# A class to preprocess all tweets (both test & training)\n",
    "class TweetSweeper:\n",
    "    def __init__(self):\n",
    "        self._at_user = 'AT_USER'\n",
    "        self._url = 'URL'\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+[self._at_user,self._url])\n",
    "        \n",
    "    # Accepts tweets as a list of dictionaries with keys, \"text\" and \"label\".\n",
    "    # Returns a list of tuples, each with a list of words and the label.\n",
    "    def tidy(self, tweets):\n",
    "        scrubbed = []\n",
    "        for tweet in tweets:\n",
    "            scrubbed.append((self._tidy(tweet['text']),tweet['label']))\n",
    "        return scrubbed\n",
    "    \n",
    "    # Accepts a tweet which is then scrubbed in several steps.\n",
    "    # Returns a tokenized list of words in the tweet, sans any stopwords.\n",
    "    def _tidy(self, tweet):\n",
    "        # 1. Convert to lower case.\n",
    "        tweet = tweet.lower()\n",
    "        # 2. Replace links with __url.\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', self._url, tweet)\n",
    "        # 3. Replace user mentions with __at_user\n",
    "        tweet = re.sub('@[^\\s]+', self._at_user, tweet)\n",
    "        # 4. Replace hashtags with the raw word. (i.e. '#word' => 'word')\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # The post-tag word is grouped '()' so it can be referred to as \\1.\n",
    "        # Finally, tokenize the tweet into a list of words...\n",
    "        tweet = word_tokenize(tweet)\n",
    "        # ...and return, minus any stopwords.\n",
    "        return [word for word in tweet if word not in self._stopwords]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitterMaid = TweetSweeper()\n",
    "cleanTrainingData = twitterMaid.tidy(trainingData)\n",
    "cleanTestData = twitterMaid.tidy(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features and train classifier. For this exercise, two methods will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NAIVE BAYES\n",
    "import nltk\n",
    "\n",
    "def buildVocab(tidyData):\n",
    "    all_words = []\n",
    "    # This gives a list where all the words in all tweets are present.\n",
    "    for (words, sentiment) in tidyData:\n",
    "        all_words.extend(words)\n",
    "    wordlist = nltk.FreqDist(all_words) # Create a dict of each word with frequency.\n",
    "    word_features = wordlist.keys() # A unique list of words.\n",
    "    return word_features\n",
    "\n",
    "# NLTK has an apply_features function that takes a user-defined function to extract features.\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "\n",
    "word_features = buildVocab(cleanTrainingData)\n",
    "trainingSet = nltk.classify.apply_features(extract_features, cleanTrainingData)\n",
    "# apply_features applies the previously defined extract_features function to each element of cleanTrainingData.\n",
    "# It automagically identifies each element as a tuple and assumes it to take the (text, label) format before\n",
    "# applying the function to the text.\n",
    "\n",
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SUPPORT VECTOR MACHINE\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Format has to be altered for CountVectorizer because it takes in the document directly and\n",
    "# builds its own vocabulary; therefore, data and labels will be fed separately (as opposed to in tuples).\n",
    "# Of note is that Naive Bays allows for N classes, while SVM is a binary classifier only.\n",
    "\n",
    "svmTrainingData = [''.join(tweet[0]) for tweet in cleanTrainingData]\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(svmTrainingData).toarray()\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "# Now, use swn to weight these features.\n",
    "swn_weights = []\n",
    "for word in vocabulary:\n",
    "    # Wrapped in a try because some words may not be in swn.\n",
    "    try:\n",
    "        synset = list(swn.senti_synsets(word))\n",
    "        common_meaning = synset[0] # use most common meaning by default.\n",
    "        if common_meaning.pos_score() > common_meaning.neg_score():\n",
    "            weight = common_meaning.pos_score()\n",
    "        elif common_meaning.pos_score() < common_meaning.neg_score():\n",
    "            weight = -(common.meaning.neg_score())\n",
    "        else:\n",
    "            weight = 0\n",
    "    except:\n",
    "        weight = 0\n",
    "    swn_weights.append(weight)\n",
    "    \n",
    "swn_X = []\n",
    "for row in X:\n",
    "    swn_X.append(np.multiply(row,np.array(swn_weights))) # convert list to a np array\n",
    "swn_X = np.vstack(swn_X)\n",
    "\n",
    "# Prepare labels\n",
    "array_labels = {'irrelevant':0, 'positive':1, 'negative':2, 'neutral':3}\n",
    "labels = [array_labels[tweet[1]] for tweet in cleanTrainingData]\n",
    "y = np.array(labels)\n",
    "\n",
    "# Finally, build the SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "SVMClassifier = SVC()\n",
    "SVMClassifier.fit(swn_X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run classifier on downloaded tweets.\n",
    "\n",
    "# Naive Bayes\n",
    "NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in cleanTestData]\n",
    "\n",
    "# SVM\n",
    "SVMResultLabels = []\n",
    "for tweet in cleanTestData:\n",
    "    tweet_sentence = ''.join(tweet[0]) # needs a full sentence\n",
    "    svmFeatures = np.multiply(vectorizer.transform([tweet_sentence]).toarray(),np.array(swn_weights))\n",
    "    # Predict returns a list of np.arrays. There's only 1 element and 1 array, so grab [0][0].\n",
    "    SVMResultLabels.append(SVMClassifier.predict(svmFeatures)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

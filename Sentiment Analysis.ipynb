{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Accept a search term from the user and download the last 100 tweets with that term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The python-twitter module is used because it has a RESTful API that returns\n",
    "# pre-parsed objects instead of raw JSON to make working with data easier.\n",
    "\n",
    "import twitter\n",
    "import json\n",
    "\n",
    "# The twitter library uses an API object authenticated with app keys to access the API.\n",
    "# For privacy reasons, these keys are loaded from a local JSON file not included in this repo.\n",
    "\n",
    "with open('twitter_keys.json') as keystore:\n",
    "    keys = json.load(keystore)\n",
    "\n",
    "api = twitter.Api(consumer_key=keys['consumer_key'],\n",
    "                  consumer_secret=keys['consumer_secret'],\n",
    "                  access_token_key=keys['access_token_key'],\n",
    "                  access_token_secret=keys['access_token_secret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a function to accept a search term and fetch the tweets with that term.\n",
    "def fetchTestData(search_string):\n",
    "    try:\n",
    "        tweets_fetched = api.GetSearch(search_string, count=100)\n",
    "        print(\"Fetched\",str(len(tweets_fetched)),\"tweets with the term\",search_string)\n",
    "        return [{'text':status.text,'label':None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print(\"Failed to fetch tweets with the term\",search_string)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. Please enter a search term: apple\n",
      "Fetched 100 tweets with the term apple\n"
     ]
    }
   ],
   "source": [
    "search_string = input(\"Hello. Please enter a search term: \")\n",
    "testData = fetchTestData(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey @Apple, I'm 35. Stop autocorrecting it to ducking. Thanks!\n",
      "Happy NYE! Wanted to give you all a little something special...my new updated emojis are on sale for .99 cents! âœ¨â€¦ https://t.co/FeQsifsvV6\n",
      "What capitulation and normalization looks like: WSJ won't call Donald Trump's lies 'lies'  https://t.co/zf4zvatiEs\n",
      "Kindly follow us and ask your follower to follow us if they use apple app #retweet # followback #weretweet\n",
      "Me gustÃ³ un video de @YouTube https://t.co/SwLSkwqyak Uncharted 4 Multiplayer - Hilarious Hook Melee Only Gameplay! (22 Downs)\n",
      "iPhoneç”¨ã€€ç„¡æ–™ã‚¢ãƒ—ãƒªã€€åŒ»å­¦éƒ¨é–€ã€€ç¬¬40ä½\n",
      "å¾ªç’°å™¨ç–¾æ‚£ã€€ãƒŠãƒ¼ã‚¹ãƒ•ãƒ«ç–¾æ‚£...\n",
      "https://t.co/NQDvn2IDuc\n",
      "çœ‹è­·å¸«ï¼ˆãƒŠãƒ¼ã‚¹ï¼‰ï¼†çœ‹è­·å­¦ç”Ÿã®ãŸã‚ã®å­¦ç¿’ãƒ»å‹‰å¼·ã‚¢ãƒ—ãƒªï¼...\n",
      "#ãƒ¡ãƒ‡ã‚£ã‚«ãƒ« #ã‚¢ãƒ—ãƒª https://t.co/0xIHoPx9iL\n",
      "Apple refugees dish on how iPhone development culture echoes into Pearl Automation https://t.co/OddbWXCSJnâ€¦ https://t.co/HgtnmkJ0fx\n",
      "RT @fe_city_boy: ÐšÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ Apple Ð¿Ñ‹Ñ‚Ð°ÐµÑ‚ÑÑ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð²ÑÐµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹ Ð±ÐµÑÐ¿Ñ€Ð¾Ð²Ð¾Ð´Ð½Ñ‹Ð¼Ð¸, Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ Ñ‡Ñ‚Ð¾ Ð´Ð¾ ÑÐ¸Ñ… Ð¿Ð¾Ñ€ Ð½Ðµ Ð½Ð°ÑƒÑ‡Ð¸Ð»Ð°ÑÑŒ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð°.\n",
      "@chadwildclay I'm #bingewatching your videos. Pen Pineapple Apple Pen is gonna be stuck in my head forever now. ðŸ–ŠðŸŽ\n"
     ]
    }
   ],
   "source": [
    "# First 10 tweets that were fetched.\n",
    "for tweet in testData[0:9]:\n",
    "    print(tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Classify these tweets as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This requires downloading a corpus of tweet data. However, twitter only allows\n",
    "# tweet ID's to be shared, and not the tweets themselves. The API can be used to\n",
    "# cross-reference the corpus, but considering that Niek Sanders corpus contains\n",
    "# 5000 tweets and twitter limits API pulls to 180/15 mins, it will take several \n",
    "# hours to complete the pull.\n",
    "\n",
    "def createTrainingCorpus(corpusFile,tweetFile):\n",
    "    import csv\n",
    "    corpus=[]\n",
    "    with open(corpusFile,'r') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',',quotechar='\"')\n",
    "        for row in lineReader:\n",
    "            corpus.append({'tweet_id':row[2],'label':row[1],'topic':row[0]})\n",
    "    # The tweet pull is self-throttled to work around twitter's rate limit.\n",
    "    import time\n",
    "    rate_limit=180\n",
    "    sleep_time=900/180 # 15 minutes / limit\n",
    "    trainingData=[]\n",
    "    downloadCount = 1\n",
    "    for tweet in corpus:\n",
    "        try:\n",
    "            status = api.GetStatus(tweet['tweet_id'])\n",
    "            tweet['text'] = status.text\n",
    "            print('fetched tweet',str(downloadCount),'of',str(len(corpus)))\n",
    "            downloadCount += 1\n",
    "            trainingData.append(tweet)\n",
    "            time.sleep(sleep_time) # to avoid rate limit\n",
    "        except:\n",
    "            continue\n",
    "    with open(tweetFile,'w') as csvfile:\n",
    "        lineWriter = csv.writer(csvfile,delimiter=',',quotechar='\"')\n",
    "        for tweet in trainingData:\n",
    "            # Ignore failures on line write so the trainingData doesn't get lost due to a single encoding error.\n",
    "            try:\n",
    "                lineWriter.writerow([tweet['tweet_id'],tweet['text'],tweet['label'],tweet['topic']])\n",
    "            except:\n",
    "                continue\n",
    "    return trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Commented out so 'Run All' doesn't kick off a 10-hour loop.\n",
    "#trainingData = createTrainingCorpus('./corpus.csv','./tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since I don't want to deal with loading 10 hours worth of tweets, this fetches the\n",
    "# training data from my pre-saved csv file.\n",
    "def loadTrainingData(tweetsFile):\n",
    "    import csv\n",
    "    with open(tweetsFile,'r') as dataFile:\n",
    "        lineReader = csv.reader(dataFile, delimiter=',', quotechar='\"')\n",
    "        trainingData = []\n",
    "        for row in lineReader:\n",
    "            if len(row) > 0: # Ignore empty rows, which would cause an 'out of range' indexing error.\n",
    "                trainingData.append({'tweet_id':row[0],'text':row[1],'label':row[2],'topic':row[3]})\n",
    "    return trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData = loadTrainingData('./tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Whit\\Anaconda3\\lib\\site-packages\\nltk\\decorators.py:59: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  regargs, varargs, varkwargs, defaults = inspect.getargspec(func)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# A class to preprocess all tweets (both test & training)\n",
    "class TweetSweeper:\n",
    "    def __init__(self):\n",
    "        self._at_user = 'AT_USER'\n",
    "        self._url = 'URL'\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+[self._at_user,self._url])\n",
    "        \n",
    "    # Accepts tweets as a list of dictionaries with keys, \"text\" and \"label\".\n",
    "    # Returns a list of tuples, each with a list of words and the label.\n",
    "    def tidy(self, tweets):\n",
    "        scrubbed = []\n",
    "        for tweet in tweets:\n",
    "            scrubbed.append((self._tidy(tweet['text']),tweet['label']))\n",
    "        return scrubbed\n",
    "    \n",
    "    # Accepts a tweet which is then scrubbed in several steps.\n",
    "    # Returns a tokenized list of words in the tweet, sans any stopwords.\n",
    "    def _tidy(self, tweet):\n",
    "        # 1. Convert to lower case.\n",
    "        tweet = tweet.lower()\n",
    "        # 2. Replace links with __url.\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', self._url, tweet)\n",
    "        # 3. Replace user mentions with __at_user\n",
    "        tweet = re.sub('@[^\\s]+', self._at_user, tweet)\n",
    "        # 4. Replace hashtags with the raw word. (i.e. '#word' => 'word')\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # The post-tag word is grouped '()' so it can be referred to as \\1.\n",
    "        # Finally, tokenize the tweet into a list of words...\n",
    "        tweet = word_tokenize(tweet)\n",
    "        # ...and return, minus any stopwords.\n",
    "        return [word for word in tweet if word not in self._stopwords]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitterMaid = TweetSweeper()\n",
    "cleanTrainingData = twitterMaid.tidy(trainingData)\n",
    "cleanTestData = twitterMaid.tidy(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run classifier on downloaded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
